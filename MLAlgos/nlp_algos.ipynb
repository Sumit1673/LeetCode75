{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "class CustomTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer splits a sentence into tokens(single word, multiple words, char, suffix or prefix).    \n",
    "    \"\"\"\n",
    "    def __init__(self, raw_sen, use_punctuation_as_tokens=True, punctuations=string.punctuation,\n",
    "                 token_boundaries=[\" \", \"-\"], token_delimeter=\"<SPLIT>\",\n",
    "                 n_grams=1):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            raw_sen (str): sentence to tokenize\n",
    "\n",
    "            punctuations (List[str]): remove a list of punctuations.\n",
    "            When to Use Punctuation as Tokens:\n",
    "            Here are some scenarios where including punctuation as tokens can be beneficial:\n",
    "            Tasks Where Punctuation is Meaningful: Sentiment analysis, sarcasm detection, parsing, and any task where\n",
    "            punctuation conveys important information.Models Designed for Punctuation: Some NLP models, like those specifically\n",
    "            trained on social media text, are designed to handle and learn from punctuation.\n",
    "            \n",
    "            When Not to Use Punctuation as Tokens:\n",
    "            Here are some scenarios where excluding punctuation might be preferable:\n",
    "            Tasks Where Punctuation is Irrelevant: Topic modeling, machine translation, or tasks where meaning primarily\n",
    "            comes from the words themselves. Large Datasets: If you have a very large dataset, the increased vocabulary size\n",
    "            due to punctuation might become an issue.\n",
    "            \n",
    "            Best Practices:\n",
    "            Experiment: The best way to determine whether to include punctuation as tokens is to experiment and evaluate model\n",
    "            performance on your specific task and dataset.\n",
    "            Fine-Tuning: If you're using a pretrained model, you might need to fine-tune it on your data to ensure it effectively\n",
    "            handles punctuation tokens.\n",
    "            Consider Frequency: If you're worried about vocabulary size, you can choose to only tokenize the most frequent\n",
    "            punctuation marks or those most relevant to your task.\n",
    "\n",
    "            token_boundaries (List[str]): split the tokens from the sentence based on token boundaries. Other \n",
    "            languages like Japanese they dont have a space to seperate words but you a caharacter from chinese\n",
    "            called as KANA to separate words. \n",
    "            \n",
    "            token_delimeter (List[str]): the delimiter token <SPLIT> is used to explicitly mark the boundaries\n",
    "                created by spaces, hyphens, and punctuation. This serves a few key purposes:\n",
    "                    Preserving Information: Delimiters can be used to retain information about the original structure\n",
    "                    of the text. For instance, the presence of a period indicates the end of a sentence, while a comma\n",
    "                    might signal a pause within a sentence. This structural information can be useful for downstream NLP\n",
    "                    tasks like parsing or syntax analysis.\n",
    "                    Disambiguation: In some cases, token boundaries might not be obvious. For example, in the text \"New York,\"\n",
    "                    is it one token (a city) or two (an adjective and a noun)? Using a delimiter between \"New\" and \"York\" could\n",
    "                    help clarify this.\n",
    "                    Feature Engineering:  Delimiters can be treated as tokens themselves, potentially serving as features for\n",
    "                    machine learning models. For example, the presence or absence of certain punctuation marks could be \n",
    "                    informative for sentiment analysis or language identification tasks.\n",
    "\n",
    "                    Sometimes Necessary:\n",
    "                    If you need to preserve information about the original text structure (e.g., for parsing or syntax analysis).\n",
    "                    When dealing with languages where word boundaries are not clearly marked by spaces (e.g., Chinese or Japanese).\n",
    "                    If your downstream tasks require specific delimiter tokens as features (e.g., for sentiment analysis).\n",
    "                    \n",
    "                    Sometimes Not Necessary:\n",
    "                    In simple word-based tokenization, where spaces naturally separate words, you might not need explicit delimiters.\n",
    "                    If your primary goal is to create a bag-of-words representation (where word order doesn't matter),\n",
    "                    delimiters might not be essential.\n",
    "        \"\"\"\n",
    "        self.raw = raw_sen\n",
    "        self._punc = punctuations\n",
    "        self.use_punc_as_tokens = use_punctuation_as_tokens # if false we will remove all the punctions # good for bag of words taks\n",
    "        self._token_boundary = token_boundaries\n",
    "        self._delimiter = token_delimeter\n",
    "        self.ngrams = n_grams\n",
    "        self._index = 0\n",
    "        self._tokenize()    \n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Tokenize --> {self.raw}\"\n",
    "    \n",
    "    def _tokenize(self):\n",
    "        \n",
    "        work_sent = self.raw\n",
    "\n",
    "        # sepearating punctions from the words\n",
    "        if self.use_punc_as_tokens:\n",
    "            for punc in self._punc:\n",
    "                work_sent = work_sent.replace(punc, \" \"+punc+\" \")\n",
    "\n",
    "        # this is not necessary if we are tokenizing english sentence, if any other language is use then we need this\n",
    "        # how the words are seperated from each other. Ex: japanese lang\n",
    "        for delimiter in self._token_boundary:\n",
    "            work_sent = work_sent.replace(delimiter, self._delimiter)\n",
    "\n",
    "        # split into tokens based on the delimiter\n",
    "        self.tokens =  [ word.strip() for word in work_sent.split(self._delimiter) if word != \"\"]\n",
    "\n",
    "        if self.ngrams > 1:\n",
    "            ngrams = []\n",
    "            for i in range(len(self.tokens)-self.ngrams+1):\n",
    "                ngrams.append(tuple(self.tokens[i:i+self.ngrams]))\n",
    "            \n",
    "            self.tokens = ngrams\n",
    "            \n",
    "\n",
    "\n",
    "    # def remove_punctuations(self, pattern=r'[^\\w\\s]'):\n",
    "    #     return re.sub(pattern, '', self.raw)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.tokens:\n",
    "            return len(self.tokens)\n",
    "        else:\n",
    "            \"No tokens present\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._index < len(self.tokens):\n",
    "            result = self.tokens[self._index]\n",
    "            self._index+=1\n",
    "            return result\n",
    "        raise StopIteration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', 'is')\n",
      "('is', 'a')\n",
      "('a', 'sentence')\n",
      "('sentence', 'with')\n",
      "('with', 'hyphens')\n",
      "('hyphens', '.')\n"
     ]
    }
   ],
   "source": [
    "dtoken = CustomTokenizer(\"This is a sentence-with-hyphens.\", n_grams=2)\n",
    "for i in dtoken:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7e16ba1ab310>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestCustomTokenizer(unittest.TestCase):\n",
    "    def test_word_tokenization(self):\n",
    "        sentence = \"This is a simple sentence.\"\n",
    "        tokenizer = CustomTokenizer(sentence)\n",
    "        self.assertEqual(tokenizer.tokens, [\"This\", \"is\", \"a\", \"simple\", \"sentence\",\".\"])\n",
    "        self.assertEqual(len(tokenizer), 6)\n",
    "\n",
    "    def test_punctuation_tokenization(self):\n",
    "        sentence = \"This is a sentence, with punctuation!\"\n",
    "        tokenizer = CustomTokenizer(sentence)\n",
    "        self.assertEqual(tokenizer.tokens, \n",
    "                         [\"This\", \"is\", \"a\", \"sentence\", \",\", \"with\", \"punctuation\", \"!\"])\n",
    "        self.assertEqual(len(tokenizer), 8)\n",
    "\n",
    "    def test_hyphen_tokenization(self):\n",
    "        sentence = \"This-is-a-sentence-with-hyphens.\"\n",
    "        tokenizer = CustomTokenizer(sentence)\n",
    "        self.assertEqual(tokenizer.tokens, \n",
    "                         [\"This\", \"is\", \"a\", \"sentence\", \"with\", \"hyphens\", \".\"])\n",
    "        self.assertEqual(len(tokenizer), 7)\n",
    "\n",
    "\n",
    "unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
